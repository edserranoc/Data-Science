{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c224223-d575-45ab-a016-82c10ad0515e",
   "metadata": {},
   "source": [
    "# Regression Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04581c37-334d-4cf7-b792-aafe95179638",
   "metadata": {},
   "source": [
    "## Overfitting problem and model validation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e736c1e-2dd4-4f77-bb79-d66c87481a16",
   "metadata": {},
   "source": [
    "**Underfitting example:**\n",
    "\n",
    "Training set: $X\\subset \\mathbb{R}$\n",
    "\n",
    "Model: $a(x)=b+w_1x$\n",
    "\n",
    "<center><img src=\"Under_fitting.png\" width=\"350\" height=\"280\"> \n",
    "    \n",
    "It is under fitted, it has inferior quality. Simple for our data, because, there is no a linear depency between $x$ and $y$.\n",
    "    \n",
    "**Good model example:**\n",
    "    \n",
    "The model fits good the target values. Describe the behaviour of the data.\n",
    "    \n",
    "Training set: $X\\subset \\mathbb{R}$\n",
    "\n",
    "Model: $a(x)=b+w_1x+\\cdots +w_4x^4$\n",
    "    \n",
    "<center><img src=\"Nice_model.png\" width=\"350\" height=\"280\"> \n",
    "\n",
    "**Overfitting example:**\n",
    "    \n",
    "Training set: $X\\subset \\mathbb{R}$\n",
    "\n",
    "Model: $a(x)=b+w_1x+\\cdots +w_{15}x^{15}$\n",
    "\n",
    "<center><img src=\"Overfitting.png\" width=\"350\" height=\"280\">\n",
    "\n",
    "It is too complex for our data, it is overfitted, and maybe it has good performance on training examples, but, it has very poor performance on new data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d58155-c318-4860-b45f-53756a5278c1",
   "metadata": {},
   "source": [
    "**Holdout set:** (Testing set)\n",
    "\n",
    "It is used to validate our model and check whether it is overfitted or not?\n",
    "\n",
    "We can take all our labeled examples and split them into two parts, training set and holdout set. We use our training set to learn our model, like classifier or regression model, and we holdout set to calculate the equality. If the loss on holdout set is not very high, then the model is good. But, if we see that loss increases on holdout set, then maybe the model is overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351ab5fe-a9d1-4ab7-9e3d-843d1579cd59",
   "metadata": {},
   "source": [
    "How to divide our dataset in two parts?\n",
    "\n",
    "In practice we divide the data in $70\\%-30\\%$ or $80\\%-20\\%$ to training set and holdout set, respectly.\n",
    "\n",
    "If the sample is small. We want to see what happens if each example is in training set and what happens if this example is a holdout set.\n",
    "\n",
    "*Cross-validation:* We split our dataset in $K$ blocks of approximately similar size (we call these blocks folds). Then, we take the firts fold as a holdout set and the rest of the folds as training set. After to this, we train the model, calculate the metrics to this holdout set. Then, repeat the proccess taking as holdout set the others folders and the training set as all of the others. And, we just take the avarage of our stimates from all iterations of this procedure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de2288-d78d-49f6-b0f5-0487b2b2de01",
   "metadata": {},
   "source": [
    "## Model Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a52bb-7851-403b-a930-23b36b4a635d",
   "metadata": {},
   "source": [
    "**Weight penalty:**\n",
    "\n",
    "- Good model weigths: $(0.634,0.918,-0.626)$\n",
    "- overfitted model weigths: $(130.0,-525.8,\\cdots,102.6)$\n",
    "\n",
    "**Loss function with weight penalty:**\n",
    "\n",
    "$$L_{\\text {reg }}(w)=L(w)+\\lambda R(w) \\rightarrow \\min _{w}$$\n",
    "- $L(w)-$ loss function (MSE, log-loss, etc.)\n",
    "- $R(w)$ - regularizer (e.g. penalizes large weights)\n",
    "- $\\lambda$ - regularization strength\n",
    "\n",
    "**$L_2$ penalty:**  $R(w)=\\|w\\|^2=\\sum x_j^2$\n",
    "\n",
    "- Drives all weights closer to zero.\n",
    "- Can be optimized with gradient methods.\n",
    "- The optimization problem is equivalen to\n",
    "\n",
    "$$\\left\\{\\begin{array}{l}\n",
    "L(w) \\rightarrow \\min _{w} \\\\\n",
    "\\text { s.t. }\\|w\\|^{2} \\leq C\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "**$L_1$ penalty:**  $R(w)=\\|w\\|_1=\\sum |w_j|$\n",
    "\n",
    "- Drives some weights exactly to zero.\n",
    "- Learns sparse models\n",
    "- Cannot be optimized with gradient methods.\n",
    "\n",
    "**Other regularization techniques:**\n",
    "- Dimensionality reduction\n",
    "- Data augmentation\n",
    "- Dropout\n",
    "- Early stopping\n",
    "- Collect more data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e6318a-ca6d-44b8-8e51-8ff05affe665",
   "metadata": {
    "tags": []
   },
   "source": [
    "# References\n",
    "\n",
    "[1] Introduction to Deep Learning - HSE University - Coursera course.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
