{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bdb13f3-99b1-4d81-9b91-0f7976ed92b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Supervised Learning\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31156ef0-aef4-457d-842a-992e92de3dd6",
   "metadata": {},
   "source": [
    "Algorithm that learn $x$ to $y$ (input to output mapping). The main key of SL is that you give your learning algorithms examples to learn from. Learn from dala labeled with the \"right answers\".\n",
    "\n",
    "You should give input $x$ and desired output label $y$.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "| Input (X) | Output (Y) | Application |\n",
    "| --- | --- | --- |\n",
    "| Email | spam? (0/1) | spam filtering |\n",
    "| Audio | text transctipts | speech recognition |\n",
    "| English | Spanish | machine translation |\n",
    "| ad, user info | click? (0/1) | online advertisementing|\n",
    "| image of phone | defect? (0/1) | visual inspection |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cd9697-353e-4d82-b8e5-9862b4813d63",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2dcbfb-f9cb-431a-9c28-e70b1f019e97",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6b5aca-b41c-485c-8e42-a3158eac1d48",
   "metadata": {},
   "source": [
    "**Model:**\n",
    "$a(x)= b + w_1x_1+\\cdots+w_dx_d$\n",
    "\n",
    "- $w_1,\\dots,w_2$ - coefficients (weights)\n",
    "- $b$ - bias\n",
    "- $d+1$ parameters.\n",
    "\n",
    "  <center><img src=\"Linear_model.png.\" width=\"350\" height=\"280\"></center>  \n",
    "    \n",
    "    \n",
    "**Loss Function:**\n",
    "How to measure model quality?\n",
    "    \n",
    "*Mean squared error:*\n",
    "    $$L(w)=\\frac{1}{l}\\sum_{i=1}^{l}(w^Tx_i-y_i)^2=\\frac{1}{l}\\|Xw-y\\|^2$$\n",
    "    \n",
    "*Fitting a model to training data:*\n",
    " \n",
    "$$\\min_{w}L(w) \\rightarrow w=(X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c87d79-7849-413c-8172-b34d6f0c89c8",
   "metadata": {},
   "source": [
    "## Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8051951f-0b2c-410e-a0eb-4597a6523a76",
   "metadata": {
    "tags": []
   },
   "source": [
    "*Binary classification:*\n",
    "- $y\\in \\{-1, 1\\}$\n",
    "- $a(x)=\\text{sign }(w^Tx)$\n",
    "- Number of parameters: $d$ $(w\\in \\mathbb{R}^d)$.\n",
    "- Model without bias.\n",
    "\n",
    "<center><img src=\"linear_md_classif.png\" width=\"350\" height=\"280\"> </center>\n",
    "\n",
    "*Multiclass classification:*\n",
    "- $y\\in \\{1,\\dots,K\\}$\n",
    "- $a(x)=arg \\max_{k\\in\\{1,\\dots,K\\}} (w_k^Tx)$\n",
    "- Number of parameters: $K\\cdot d$ $(w_k\\in \\mathbb{R}^d)$.\n",
    "- *Example:* $z=(7,-7,5,10)$ - scores $a(x)=3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5edfbf9-2fc4-4c9d-b697-8063da574643",
   "metadata": {},
   "source": [
    "### Classification loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d65829-17eb-4602-ad2d-63bf173c3421",
   "metadata": {},
   "source": [
    "- Classification accuracy:\n",
    "$$\\frac{1}{\\ell} \\sum_{i=1}^{\\ell}\\left[a\\left(x_{i}\\right)=y_{i}\\right]$$\n",
    "    - Not differentiable\n",
    "    - Doesn't assess model confidence\n",
    "    - [P] - Iverson bracket:\n",
    "    \\begin{align*}\n",
    "    [P]= \\begin{cases}1, & P \\text { is true } \\\\ 0, & P \\text { is false }\\end{cases}\n",
    "    \\end{align*}\n",
    "- Mean Square error:\n",
    "\n",
    "    Consider an example $x_i$ such that $y_i=1$\n",
    "    Squared loss: $(w^Tx_i-1)^2$   \n",
    "   - If $a(x_i)=1$, then the loss is zero.\n",
    "   - If $a(x_i)=r$, with $r\\in [0,1]$, it is inconfident in its decision and we penalize if for ***low confidence***.\n",
    "   - If $a(x_i)=r$, with $r\\in (-\\infty,0]$, then it misclassified, so we give it an larger penalty.\n",
    "   - If $a(x_i)=r$, with $r\\in (1,\\infty]$, then it misclassified, so we give it an larger penalty, then we penalize it for hihg congidence.\n",
    "   \n",
    "   \n",
    "<center><img src=\"Penalties.png\" width=\"350\" height=\"280\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6f574-0550-4cc6-97ad-277eefd440ec",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29034cad-1234-4004-b638-f6a11d0bccae",
   "metadata": {},
   "source": [
    "**Class probabilities:**\n",
    "\n",
    "Class scores (logits) from a lineal model:\n",
    "\n",
    "$$z=(w_1^Tx,\\dots,w_1^Tx)$$\n",
    "\n",
    "Convert to probabilities, to distribution\n",
    "\n",
    "1. $(e^{z_1},\\cdots,e^{z_1})$\n",
    "2. Compute the softmax transform (It's a probability function) \n",
    "$$\\sigma(z)=\\left( \\frac{e^{z_1}}{\\sum e^{z_k}},\\cdots,\\frac{e^{z_K}}{\\sum e^{z_k}} \\right)$$\n",
    "\n",
    "*Example:* $z=(7,-7.5,10)$, $\\sigma(z)\\approx (0.05,0,0.95)$ \n",
    "\n",
    "**Predicted class probabilities (model output):**\n",
    "$$\\sigma(z)=\\left( \\frac{e^{z_1}}{\\sum e^{z_k}},\\cdots,\\frac{e^{z_K}}{\\sum e^{z_k}} \\right)$$\n",
    "\n",
    "-*Target Values* for class probabilities:\n",
    "$$p=([y=1],\\dots,[y=K])$$\n",
    "\n",
    "- *Measure the distance between probability distributions* \n",
    "    - Cross entropy: The intuition is we consider a target distribution $P$ and an   approximation of the target distribution $Q$, then the cross-entropy of $Q$ from $P$ is the number of additional bits to represent an event using $Q$ instead of $P$.\n",
    "    \n",
    "    $$H(P,Q)=-\\sum_{x\\in X}P(x)\\log{(Q(x))}$$\n",
    "    \n",
    "    *Note:* $\\log$ is the base-$2$ logarithm, meaning that the results are in bits. If the base-$e$, the result will have the units called nats.\n",
    "    \n",
    "    - Similiarity between $\\sigma$ and $p$ can be measured by the cross-entropy:\n",
    "    $$H(p,\\sigma)=\\sum_{k=1}^{K}[y=k] \\log \\frac{e^{z_{k}}}{\\sum_{j=1}^{K} e^{z_{j}}}=-\\log \\frac{e^{z_{y}}}{\\sum_{j=1}^{K} e^{z_{j}}}$$\n",
    "    \n",
    "        Example: Suppose $K=3$ and $y=1$: \n",
    "\n",
    "        - $p=(1,0,0)$, then $H(p,\\sigma)=0$\n",
    "        - $p=(0.5,0.25,0.25)$, then $H(p,\\sigma)\\approx 0.693$\n",
    "        - $p=(0,1,0)$, then $H(p,\\sigma)= +\\infty$\n",
    "        \n",
    "    *Note:* Cross entropy gives high penalty for models that are confident in wrong decisions.\n",
    "    \n",
    "    - Cross-entropy is diferrenciable and can be used as a loss function. \n",
    "    \n",
    "\\begin{aligned}\n",
    "L(w, b) &=-\\sum_{i=1}^{\\ell} \\sum_{k=1}^{K}\\left[y_{i}=k\\right] \\log \\frac{e^{w_{k}^{T} x_{i}}}{\\sum_{j=1}^{K} e^{w_{j}^{T} x_{i}}} \\\\\n",
    "&=-\\sum_{i=1}^{\\ell} \\log \\frac{e^{w_{y_{i}}^{T} x_{i}}}{\\sum_{j=1}^{K} e^{w_{j}^{T} x_{i}}} \\rightarrow \\min _{w}\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a712ce62-8c4f-4165-869c-68a5f8453661",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "- It's known from machine learning that models with high confidence generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8234c2f-90b5-45f8-9cbf-cbd0cd4d97b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# References\n",
    "\n",
    "[1] Introduction to Deep Learning - HSE University - Coursera course.\n",
    "\n",
    "[2] Supervised Machine Learning: Regression and Classification - OpenIA, Stanford University - Coursera course.\n",
    "\n",
    "[3] https://machinelearningmastery.com/cross-entropy-for-machine-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
