{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77388872-79d3-4f79-9d71-6c5a554d7fe5",
   "metadata": {},
   "source": [
    "# Algorithms to optimize differentiable functions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f33ddec-d968-44a8-b044-b013b70540a7",
   "metadata": {},
   "source": [
    "## Gradiant Descent\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d42798-8427-4d3b-a3cd-caf381cd6e5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Optimization problem:** \n",
    "\n",
    "$$L(w) \\rightarrow \\min_{w}$$\n",
    "\n",
    "Suppose we have some approximation $w^0$- how to refine it?\n",
    "\n",
    "- $w^0$ - initialization.\n",
    "- $\\nabla L(w^0)$ - gradient vector.\n",
    "    - Points in the direction of the steepest slope at $w^0$.\n",
    "    - The function has fastest decrease rate in the direction of negative gradient.\n",
    "\n",
    "- $w^{1}=w^{0}-\\eta_1 \\nabla L(w^0)$\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "$w^0$ - initialization.\n",
    "\n",
    "while True:\n",
    "\n",
    ">$w^{t}=w^{t-1}-\\eta_{t} \\nabla L\\left(w^{t-1}\\right)$\n",
    "\n",
    ">if $\\left\\|w^{t}-w^{t-1}\\right\\|<\\epsilon$ then break\n",
    "\n",
    "\n",
    "**Advantages:**\n",
    "- Easy to implement\n",
    "- Very general, can be applied to any differentiable loss function.\n",
    "- Requiress less memory and computations (for stochastic methods)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b203df87-df92-4624-b2e9-6b0c597a4bc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stochastic Gradiant Descent\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e384b5-ff25-4841-8c6c-5ad1c20b4190",
   "metadata": {},
   "source": [
    "**Optimization problem:**\n",
    "\\begin{align*}\n",
    "L(w)=\\sum_{i=1}^{\\ell} L\\left(w ; x_{i}, y_{i}\\right) \\rightarrow \\min _{w}\n",
    "\\end{align*}\n",
    "\n",
    "$w^{0}-$ initialization\n",
    "\n",
    "while True:\n",
    "> $i=$ random index between 1 and $\\ell$\n",
    "\n",
    "> $w^{t}=w^{t-1}-\\eta_{t} \\nabla L\\left(w^{t-1} ; x_{i} ; y_{i}\\right)$\n",
    "\n",
    "> if $\\left\\|w^{t}-w^{t-1}\\right\\|<\\epsilon$ then break\n",
    "\n",
    "**Disadvantages:**\n",
    "- Noisy updates lead to fluctuations\n",
    "- Needs only one example on each step\n",
    "- Can be used in online setting\n",
    "- Learning rate $\\eta_{t}$ should be chosen very carefully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ce9ef2-8412-4516-9668-fcc1a4f80539",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradiant Descent\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59eb555-e294-414d-95e2-a0cdd5b2efb6",
   "metadata": {},
   "source": [
    "Optimization problem:\n",
    "\n",
    "\\begin{align*}\n",
    "L(w)=\\sum_{i=1}^{\\ell} L\\left(w ; x_{i}, y_{i}\\right) \\rightarrow \\min _{w}\n",
    "\\end{align*}\n",
    "\n",
    "$w^{0}-$ initialization\n",
    "\n",
    "while True:\n",
    ">$i_{1}, \\ldots, i_{m}=$ random indices between 1 and $\\ell$\n",
    "\n",
    "> $w^{t}=w^{t-1}-\\eta_{t} \\frac{1}{m} \\sum_{j=1}^{m} \\nabla L\\left(w^{t-1} ; x_{i_{j}} ; y_{i_{j}}\\right)$\n",
    "\n",
    ">if $\\left\\|w^{t}-w^{t-1}\\right\\|<\\epsilon$ then break\n",
    "\n",
    "**Comments:**\n",
    "\n",
    "- Still can be used in online setting\n",
    "- Reduces the variance of gradient approximations\n",
    "- Learning rate $\\eta_{t}$ should be chosen very carefully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9d53ae-f03f-45ee-9a39-6e93d22d0cc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gradient descent extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba099372-8b2f-4bcb-96d1-c2f0d63ee69d",
   "metadata": {},
   "source": [
    "**Momentum:**\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{gathered}\n",
    "h_{t}=\\alpha h_{t-1}+\\eta_{t} g_{t} \\\\\n",
    "w^{t}=w^{t-1}-h_{t}\n",
    "\\end{gathered}\n",
    "\\end{align*}\n",
    "- Tends to move in the same direction as on previous steps\n",
    "- $h_{t}$ accumulates values along dimensions where gradients have the same sign\n",
    "- Usually: $\\alpha=0.9$\n",
    "\n",
    "**Nesterov momentum:**\n",
    "\n",
    "First, step in the direction of ht to get some new approximation of parameter vector. And then to calculate gradient at the new point, w t-1 plus ht. So in this case, we'll get a better approximation on the next step\n",
    "\n",
    "\\begin{gathered}\n",
    "h_{t}=\\alpha h_{t-1}+\\eta_{t} \\nabla L\\left(w^{t-1}-\\alpha h_{t-1}\\right) \\\\\n",
    "w^{t}=w^{t-1}-h_{t}\n",
    "\\end{gathered}\n",
    "\n",
    "**AdaGrad:**\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{gathered}\n",
    "G_{j}^{t}=G_{j}^{t-1}+g_{t j}^{2} \\\\\n",
    "w_{j}^{t}=w_{j}^{t-1}-\\frac{\\eta_{t}}{\\sqrt{G_{j}^{t}+\\epsilon}} g_{t j}\n",
    "\\end{gathered}\n",
    "\\end{align*}\n",
    "- $g_{t j}$ - gradient with respect to $j$-th parameter\n",
    "- Separate learning rates for each dimension\n",
    "- Suits for sparse data\n",
    "- Learning rate can be fixed: $\\eta_{t}=0.01$\n",
    "- $G_j^t$ always increases, leads to early stops\n",
    "- Parameter $G$ can become too large (Accumulates squares of gradients)\n",
    "\n",
    "**RMSprop:**\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{aligned}\n",
    "G_{j}^{t} &=\\alpha G_{j}^{t-1}+(1-\\alpha) g_{t j}^{2} \\\\\n",
    "w_{j}^{t} &=w_{j}^{t-1}-\\frac{\\eta_{t}}{\\sqrt{G_{j}^{t}+\\epsilon}} g_{t j}\n",
    "\\end{aligned}\n",
    "\\end{align*}\n",
    "- $\\alpha$ is about $0.9$\n",
    "- Learning rate adapts to latest gradient steps\n",
    "\n",
    "**Adam**\n",
    "\\begin{align*}\n",
    "\\begin{gathered}\n",
    "m_{j}^{t}=\\frac{\\beta_{1} m_{j}^{t-1}+\\left(1-\\beta_{1}\\right) g_{t j}}{1-\\beta_{1}^{t}} \\\\\n",
    "v_{j}^{t}=\\frac{\\beta_{2} v_{j}^{t-1}+\\left(1-\\beta_{2}\\right) g_{t j}^{2}}{1-\\beta_{2}^{t}} \\\\\n",
    "w_{j}^{t}=w_{j}^{t-1}-\\frac{\\eta_{t}}{\\sqrt{v_{j}^{t}}+\\epsilon} m_{j}^{t}\n",
    "\\end{gathered}\n",
    "\\end{align*}\n",
    "Combines momentum and individual learning rates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
